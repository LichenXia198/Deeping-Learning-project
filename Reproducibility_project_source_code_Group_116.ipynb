{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Reproducibility project source code Group 116.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPvxRQzOMDLdb84sM+mOAz2",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LichenXia198/Deeping-Learning-project/blob/main/Reproducibility_project_source_code_Group_116.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YbN1LSywVYdu"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6xBZIu1tW9X9"
      },
      "source": [
        "# Setup\n",
        "from sklearn.cluster import MeanShift\n",
        "import torch\n",
        "import math\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from scipy.spatial.distance import cdist \n",
        "!pip install POT\n",
        "from scipy.io import loadmat\n",
        "import ot\n",
        "from torch.utils.data import TensorDataset, DataLoader"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o6Qt35eOLvS3"
      },
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k_jRPDNxIgSl"
      },
      "source": [
        "#preprocess datasets\n",
        "#process amazon datset\n",
        "amazon = loadmat('amazon_decaf.mat')\n",
        "#Normalization\n",
        "norm = np.linalg.norm(amazon[\"feas\"])\n",
        "amazon_feas = torch.from_numpy(amazon[\"feas\"]/norm)\n",
        "amazon_labels = torch.zeros(len(amazon[\"labels\"]),10)\n",
        "#convert label to one-hot key\n",
        "for i in range(len(amazon[\"labels\"])):\n",
        "  amazon_labels[i][amazon[\"labels\"][i][0]-1] = 1\n",
        "#Ramdomize the order of samples\n",
        "idx = torch.randperm(amazon_feas.shape[0])\n",
        "amazon_feas = amazon_feas[idx].view(amazon_feas.size())\n",
        "amazon_labels = amazon_labels[idx].view(amazon_labels.size())\n",
        "#split dataset\n",
        "train = math.ceil(amazon_feas.shape[0]*0.7)\n",
        "vaild = math.floor(amazon_feas.shape[0]*0.2)\n",
        "test = amazon_feas.shape[0]-train-vaild\n",
        "amazon_feas, amazon_feas_v, amazon_feas_t = torch.split(amazon_feas, [train,vaild,test])\n",
        "amazon_labels, amazon_labels_v, amazon_labels_t = torch.split(amazon_labels, [train,vaild,test])\n",
        "#################\n",
        "\n",
        "#process caltech datset\n",
        "caltech = loadmat('caltech_decaf.mat')\n",
        "#Normalization\n",
        "norm = np.linalg.norm(caltech[\"feas\"])\n",
        "caltech_feas = torch.from_numpy(caltech[\"feas\"]/norm)\n",
        "caltech_labels = torch.zeros(len(caltech[\"labels\"]),10)\n",
        "#convert label to one-hot key\n",
        "for i in range(len(caltech[\"labels\"])):\n",
        "  caltech_labels[i][caltech[\"labels\"][i][0]-1] = 1\n",
        "#Ramdomize the order of samples\n",
        "idx = torch.randperm(caltech_feas.shape[0])\n",
        "caltech_feas = caltech_feas[idx].view(caltech_feas.size())\n",
        "caltech_labels = caltech_labels[idx].view(caltech_labels.size())\n",
        "#split dataset\n",
        "train = math.ceil(caltech_feas.shape[0]*0.7)\n",
        "vaild = math.floor(caltech_feas.shape[0]*0.2)\n",
        "test = caltech_feas.shape[0]-train-vaild\n",
        "caltech_feas, caltech_feas_v, caltech_feas_t = torch.split(caltech_feas, [train,vaild,test])\n",
        "caltech_labels, caltech_labels_v, caltech_labels_t = torch.split(caltech_labels, [train,vaild,test])\n",
        "\n",
        "###############\n",
        "\n",
        "#process dslr datset\n",
        "dslr = loadmat('dslr_decaf.mat')\n",
        "#Normalization\n",
        "norm = np.linalg.norm(dslr[\"feas\"])\n",
        "dslr_feas = torch.from_numpy(dslr[\"feas\"]/norm)\n",
        "dslr_labels = torch.zeros(len(dslr[\"labels\"]),10)\n",
        "#convert label to one-hot key\n",
        "for i in range(len(dslr[\"labels\"])):\n",
        "  dslr_labels[i][dslr[\"labels\"][i][0]-1] = 1\n",
        "#Ramdomize the order of samples\n",
        "idx = torch.randperm(dslr_feas.shape[0])\n",
        "dslr_feas = dslr_feas[idx].view(dslr_feas.size())\n",
        "dslr_labels = dslr_labels[idx].view(dslr_labels.size())\n",
        "#split dataset\n",
        "train = math.ceil(dslr_feas.shape[0]*0.7)\n",
        "vaild = math.floor(dslr_feas.shape[0]*0.2)\n",
        "test = dslr_feas.shape[0]-train-vaild\n",
        "dslr_feas, dslr_feas_v, dslr_feas_t = torch.split(dslr_feas, [train,vaild,test])\n",
        "dslr_labels, dslr_labels_v, dslr_labels_t = torch.split(dslr_labels, [train,vaild,test])\n",
        "\n",
        "###############\n",
        "\n",
        "#process webcam datset\n",
        "webcam = loadmat('webcam_decaf.mat')\n",
        "#Normalization\n",
        "norm = np.linalg.norm(webcam[\"feas\"])\n",
        "webcam_feas = torch.from_numpy(webcam[\"feas\"]/norm)\n",
        "webcam_labels = torch.zeros(len(webcam[\"labels\"]),10)\n",
        "#convert label to one-hot key\n",
        "for i in range(len(webcam[\"labels\"])):\n",
        "  webcam_labels[i][webcam[\"labels\"][i][0]-1] = 1\n",
        "#Ramdomize the order of samples\n",
        "idx = torch.randperm(webcam_feas.shape[0])\n",
        "webcam_feas = webcam_feas[idx].view(webcam_feas.size())\n",
        "webcam_labels = webcam_labels[idx].view(webcam_labels.size())\n",
        "#split dataset\n",
        "train = math.ceil(webcam_feas.shape[0]*0.7)\n",
        "vaild = math.floor(webcam_feas.shape[0]*0.2)\n",
        "test = webcam_feas.shape[0]-train-vaild\n",
        "webcam_feas, webcam_feas_v, webcam_feas_t = torch.split(webcam_feas, [train,vaild,test])\n",
        "webcam_labels, webcam_labels_v, webcam_labels_t = torch.split(webcam_labels, [train,vaild,test])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bbIoL48AH62i"
      },
      "source": [
        "# combine sourse datasets\n",
        "all_features = torch.cat((webcam_feas,amazon_feas,dslr_feas))\n",
        "all_labels = torch.cat((webcam_labels,amazon_labels,dslr_labels))\n",
        "print(all_features.shape)\n",
        "print(all_labels.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ydx8Z7vo8Am"
      },
      "source": [
        "#One layer network (classifer needs to be optimized)\n",
        "class MyNet(nn.Module):\n",
        "    \n",
        "    \n",
        "    def __init__(self):\n",
        "        super(MyNet, self).__init__()\n",
        "        self.fc1 = nn.Linear(4096, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h = F.relu(self.fc1(x))\n",
        "        return h\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-nuNY_ud8PPt"
      },
      "source": [
        "# Tensorboard\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from torchsummary import summary\n",
        "\n",
        "#Additional Setup to use Tensorboard\n",
        "!pip install -q tensorflow\n",
        "%load_ext tensorboard\n",
        "# Open Tensorboard\n",
        "%tensorboard --logdir runs/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i4-7hfN1I_gn"
      },
      "source": [
        "writer = SummaryWriter()\n",
        "\n",
        "# Calculate Wasserste Distance\n",
        "def my_loss_custom(beta,G,C0,target_predict,y):\n",
        "  loss = torch.sum(torch.square(torch.cdist(y,target_predict))*G)\n",
        "  C = beta*C0 + torch.square(torch.cdist(y,target_predict))\n",
        "  lose = C*G\n",
        "  return torch.sum(lose)\n",
        "\n",
        "# train and test JDOT with test set/validation set\n",
        "def train_jodt(le,wd,all_features,target,target_t,all_labels,target_labels,target_labels_t,writer):\n",
        "  n1 = list(all_features.size())[0]\n",
        "  ntest =list(target.size())[0]\n",
        "  wa1=np.ones((n1,))/n1\n",
        "  wb=np.ones((ntest,))/ntest \n",
        "  y1 = all_labels\n",
        "  orign = target_labels\n",
        "  orign_test = target_labels_t\n",
        "  target = target.float()\n",
        "  target_test=target_t\n",
        "  target_test = target_test.float()\n",
        "  all_features = all_features.float()\n",
        "  all_labels = all_labels.float()\n",
        "\n",
        "  C1=torch.square(torch.cdist(all_features,target))\n",
        "  beta1 = 1/torch.max(C1).data.item()\n",
        "  C1=C1/torch.max(C1)\n",
        "\n",
        "  f = MyNet()\n",
        "\n",
        "  optimizer = optim.Adam(f.parameters(),lr=le,betas=(0.9,0.99),weight_decay=wd ,eps=math.exp(-8))\n",
        "  \n",
        "  ave_train_loss = 0\n",
        "  ave_test_loss = 0\n",
        "  ave_train_acc = 0\n",
        "  ave_test_acc = 0\n",
        "  convergence = 0\n",
        "  previous_loss = torch.zeros(1)\n",
        "  k = 0\n",
        "\n",
        "  while True:\n",
        "  \n",
        "    tp = f(target)\n",
        "    \n",
        "    # Calculte Optimal Transport matrix\n",
        "    C_1 = beta1*C1+ torch.square((torch.cdist(y1,tp)))\n",
        "    G1_solution=ot.emd(wa1,wb,C_1.detach().numpy())\n",
        "    G1=torch.from_numpy(G1_solution)\n",
        "  \n",
        "    optimizer.zero_grad()\n",
        "    # Calculate Wasserste Distance\n",
        "    lose1 = my_loss_custom(beta1,G1,C1,tp,y1)\n",
        "    lose = lose1\n",
        "    lose.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    #calculate test lose test accurcy, train lose and train accurcy\n",
        "    trainloss = torch.mean((tp - orign) ** 2)\n",
        "    traincorrect = torch.sum(torch.argmax(tp, axis=1) == torch.argmax(orign, axis=1))/len(tp)\n",
        "\n",
        "    test_predict = f(target_test)\n",
        "    testloss = torch.mean((test_predict - orign_test) ** 2)\n",
        "    testcorrect = torch.sum(torch.argmax(test_predict, axis=1) == torch.argmax(orign_test, axis=1))/len(test_predict)\n",
        "\n",
        "    ave_train_loss += trainloss\n",
        "    ave_train_acc += traincorrect.item()\n",
        "\n",
        "    ave_test_loss += testloss\n",
        "    ave_test_acc += testcorrect.item()\n",
        "  \n",
        "    # print test lose test accurcy, train lose and train accurcy in tensorboard\n",
        "    if k%10 ==0 :\n",
        "      train_loss = ave_train_loss/10\n",
        "      test_loss = ave_test_loss/10\n",
        "      writer.add_scalars('Loss', {'Train': train_loss, 'Test':test_loss}, k)\n",
        "      ave_train_loss = 0\n",
        "      ave_test_loss = 0\n",
        "      train_acc = ave_train_acc/10\n",
        "      test_acc = ave_test_acc/10\n",
        "      writer.add_scalars('Accuracy',{'Train': train_acc,'Test':test_acc} , k)\n",
        "      ave_train_acc = 0\n",
        "      ave_test_acc = 0\n",
        "    \n",
        "    # Check if convergent\n",
        "    if k%10 ==0:\n",
        "      if previous_loss.data.item() -  lose.data.item() <= 0.0001:\n",
        "        convergence+=1\n",
        "      else:\n",
        "        convergence = 0\n",
        "      previous_loss = lose.detach().clone()\n",
        "    if convergence>5:\n",
        "      break\n",
        "    k+=1\n",
        "\n",
        "\n",
        "\n",
        "  # Calculate final test accurcy and print\n",
        "  test_predict = f(target_test)\n",
        "  correct = torch.argmax(test_predict, axis=1) == torch.argmax(orign_test, axis=1)\n",
        "  print(\"jjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjj\")\n",
        "  print(\"convergent!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\")\n",
        "  print(torch.sum(correct)/len(test_predict))\n",
        "  print(\"jjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjj\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S4uDUt-DXN77"
      },
      "source": [
        "# an example for training and testing JDOT\n",
        "train_jodt(0.06,0.0002,all_features,caltech_feas,caltech_feas_t,all_labels,caltech_labels,caltech_labels_t,writer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ZNhwSdrpsX5"
      },
      "source": [
        "writer = SummaryWriter()\n",
        "\n",
        "# Nomalize alphas\n",
        "def normalize_alphas_inplace(alphas):\n",
        "  alpha1, alpha2, alpha3 = alphas\n",
        "  alpha1 = torch.clamp_(alpha1, min=0)\n",
        "  alpha2 = torch.clamp_(alpha2, min=0)\n",
        "  alpha3 = torch.clamp_(alpha3, min=0)\n",
        "  sum = alpha1+alpha2+alpha3\n",
        "  alpha1 = alpha1.div_(sum)\n",
        "  alpha2 = alpha2.div_(sum)\n",
        "  alpha3 = alpha3.div_(sum)\n",
        "  return alpha1, alpha2, alpha3\n",
        "\n",
        "\n",
        "\n",
        "# train and test WJDOT with test set/validation set\n",
        "def train(lerate,wd,lra,source1,source2,source3,target,source1_labels,source2_labels,source3_labels,target_labels,target_labels_t,target_t,writer):\n",
        "  \n",
        "  #Prepare datasets\n",
        "  source1 = source1.float()\n",
        "  source2 = source2.float()\n",
        "  source3 = source3.float()\n",
        "  target = target.float()\n",
        "  n1 = list(source1.size())[0]\n",
        "  n2 = list(source2.size())[0]\n",
        "  n3 = list(source3.size())[0]\n",
        "  ntest =list(target.size())[0]\n",
        "  wa1=np.ones((n1,))/n1\n",
        "  wa2=np.ones((n2,))/n2\n",
        "  wa3=np.ones((n3,))/n3\n",
        "  wb=np.ones((ntest,))/ntest \n",
        "  y1 = source1_labels\n",
        "  y2 = source2_labels\n",
        "  y3 = source3_labels\n",
        "  orign = target_labels\n",
        "  orign_test = target_labels_t\n",
        "\n",
        "  target_test=target_t\n",
        "  target_test = target_test.float()\n",
        "\n",
        "  # Initialize alphas\n",
        "  alpha1 = torch.ones(1)/3\n",
        "  alpha2 = torch.ones(1)/3\n",
        "  alpha3 = torch.ones(1)/3\n",
        "  alpha1.requires_grad = True\n",
        "  alpha2.requires_grad = True\n",
        "  alpha3.requires_grad = True\n",
        "\n",
        "  # Initialize embedding discrepancies and beta\n",
        "  C1=torch.square(torch.cdist(source1,target))\n",
        "  beta1 = 1/torch.max(C1).data.item()\n",
        "  C1=C1/torch.max(C1)\n",
        "\n",
        "  C2=torch.square(torch.cdist(source2,target))\n",
        "  beta2 = 1/torch.max(C2).data.item()\n",
        "  C2=C2/torch.max(C2)\n",
        "\n",
        "  C3=torch.square(torch.cdist(source3,target))\n",
        "  beta3 = 1/torch.max(C3).data.item()\n",
        "  C3=C3/torch.max(C3)\n",
        "  beta_b = (beta1+beta2+beta3)/3\n",
        "\n",
        "\n",
        "  # Initialize model\n",
        "  f = MyNet()\n",
        "\n",
        "  optimizer = optim.Adam(f.parameters(),lr=lerate,betas=(0.9,0.99),weight_decay = wd,eps=math.exp(-8))\n",
        "  optimizerA = optim.Adam([alpha1, alpha2, alpha3],lr=lra,betas=(0.9,0.99),eps=math.exp(-8))\n",
        "\n",
        "  convergence = 0\n",
        "  ave_train_loss = 0\n",
        "  ave_test_loss = 0\n",
        "  ave_train_acc = 0\n",
        "  ave_test_acc = 0\n",
        "  previous_loss = torch.zeros(1)\n",
        "  k = 0\n",
        "\n",
        "  # Start training\n",
        "  while True:\n",
        "  \n",
        "    tp = f(target)\n",
        "  \n",
        "    # Normalize alphas\n",
        "    with torch.no_grad():\n",
        "      alpha1, alpha2, alpha3 = normalize_alphas_inplace((alpha1, alpha2, alpha3))\n",
        "    \n",
        "    # Overall Beta\n",
        "    beta_b = alpha1*beta1 + alpha2*beta2 + alpha3*beta3\n",
        "\n",
        "    # Optimal transport matrix for each source domain with target domain\n",
        "    C_1 = beta_b*C1+ torch.square((torch.cdist(y1,tp)))\n",
        "    G1_solution=ot.emd(wa1,wb,C_1.detach().numpy())\n",
        "    G1=torch.from_numpy(G1_solution)\n",
        "\n",
        "\n",
        "    C_2 = beta_b*C2+ torch.square((torch.cdist(y2,tp)))  \n",
        "    G2_solution=ot.emd(wa2,wb,C_2.detach().numpy())\n",
        "    G2=torch.from_numpy(G2_solution)\n",
        "  \n",
        "\n",
        "    C_3 = beta_b*C3+ torch.square((torch.cdist(y3,tp)))  \n",
        "    G3_solution=ot.emd(wa3,wb,C_3.detach().numpy())\n",
        "    G3=torch.from_numpy(G3_solution)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    optimizerA.zero_grad()\n",
        "\n",
        "    # Wasserste Distance for each source domain\n",
        "    lose1 = my_loss_custom(beta_b,G1,C1,tp,y1)\n",
        "    lose2 = my_loss_custom(beta_b,G2,C2,tp,y2)\n",
        "    lose3 = my_loss_custom(beta_b,G3,C3,tp,y3)\n",
        "\n",
        "    # Overall Wasserste Distance\n",
        "    lose = (alpha1*lose1 + alpha2*lose2+ alpha3*lose3)\n",
        "\n",
        "    lose.backward()\n",
        "    optimizer.step()\n",
        "    optimizerA.step()\n",
        "    if k%10 == 0:\n",
        "      # every 10 iterations check if convergent\n",
        "      if previous_loss.data.item() -  lose.data.item() <= 0.001:\n",
        "        convergence+=1\n",
        "      else:\n",
        "        convergence = 0\n",
        "      previous_loss = lose.detach().clone()\n",
        "\n",
        "      tp_t = tp.detach().clone()\n",
        "      y1_t = y1.detach().clone()\n",
        "      y2_t = y2.detach().clone()\n",
        "      y3_t = y3.detach().clone()\n",
        "      l1_t = torch.sum((1)*(torch.square((torch.cdist(y1_t,tp_t))))*(G1.detach().clone()))\n",
        "      l2_t = torch.sum((1)*(torch.square((torch.cdist(y2_t,tp_t))))*(G2.detach().clone()))\n",
        "      l3_t = torch.sum((1)*(torch.square((torch.cdist(y3_t,tp_t))))*(G3.detach().clone()))\n",
        "      C1_t = torch.sum(beta_b*(C1.detach().clone())*(G1.detach().clone()))\n",
        "      C2_t = torch.sum(beta_b*(C2.detach().clone())*(G2.detach().clone()))\n",
        "      C3_t = torch.sum(beta_b*(C3.detach().clone())*(G3.detach().clone()))\n",
        "\n",
        "      #print Wasserste Distance\n",
        "      print(\"1: \")\n",
        "      print(C1_t+l1_t)\n",
        "      print(\"2: \")\n",
        "      print(C2_t+l2_t)\n",
        "      print(\"3: \")\n",
        "      print(C3_t+l3_t)\n",
        "      print(\"##############\")\n",
        "      #print Alpha\n",
        "      print(\"111\")\n",
        "      print(alpha1)\n",
        "      print(\"222\")\n",
        "      print(alpha2)\n",
        "      print(\"333\")\n",
        "      print(alpha3)\n",
        "      print(\"+++++++++++++++++++++\")\n",
        "\n",
        "    #calculate test lose test accurcy, train lose and train accurcy    \n",
        "    trainloss = torch.mean((tp - orign) ** 2)\n",
        "    traincorrect = torch.sum(torch.argmax(tp, axis=1) == torch.argmax(orign, axis=1))/len(tp)\n",
        "\n",
        "    test_predict = f(target_test)\n",
        "    testloss = torch.mean((test_predict - orign_test) ** 2)\n",
        "    testcorrect = torch.sum(torch.argmax(test_predict, axis=1) == torch.argmax(orign_test, axis=1))/len(test_predict)\n",
        "\n",
        "    ave_train_loss += trainloss\n",
        "    ave_train_acc += traincorrect.item()\n",
        "\n",
        "    ave_test_loss += testloss\n",
        "    ave_test_acc += testcorrect.item()\n",
        "  \n",
        "    # print test lose test accurcy, train lose and train accurcy in tensorboard\n",
        "    if k%10 ==0 :\n",
        "      train_loss = ave_train_loss/10\n",
        "      test_loss = ave_test_loss/10\n",
        "      writer.add_scalars('WLoss', {'Train': train_loss, 'Test':test_loss}, k)\n",
        "      writer.add_scalars('WassersteinLoss', {'Train': lose}, k)\n",
        "      ave_train_loss = 0\n",
        "      ave_test_loss = 0\n",
        "      train_acc = ave_train_acc/10\n",
        "      test_acc = ave_test_acc/10\n",
        "      writer.add_scalars('WAccuracy',{'Train': train_acc,'Test':test_acc} , k)\n",
        "      ave_train_acc = 0\n",
        "      ave_test_acc = 0\n",
        "\n",
        "    if convergence>5:\n",
        "      break\n",
        "    k+=1\n",
        "\n",
        "\n",
        "  # Calculate final test accurcy and print\n",
        "  test_predict = f(target_test)\n",
        "  testloss = torch.mean((test_predict - orign_test) ** 2)\n",
        "  correct = torch.argmax(test_predict, axis=1) == torch.argmax(orign_test, axis=1)\n",
        "  print(\"############################\")\n",
        "  print(\"convergent!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\")\n",
        "  print(torch.sum(correct)/len(test_predict))\n",
        "  print(\"############################\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GCxCrc2lpUjp"
      },
      "source": [
        "# an example for training and testing WJDOT\n",
        "train(0.06,0.0002,0.001,amazon_feas,dslr_feas,caltech_feas,webcam_feas,amazon_labels,dslr_labels,caltech_labels,webcam_labels,webcam_labels_t,webcam_feas_t,writer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TKwrXQ0ayjm5"
      },
      "source": [
        "# a demonstration of tune hyperparameters\n",
        "lerates = [0.03,0.06,0.09,0.001,0.003,0.006,0.009,0.0001,0.0003,0.0006,0.0009,0.4,0.8,0.04,0.08,0.004,0.008,0.0004,0.0008]\n",
        "wds = [0.03,0.06,0.09,0.001,0.003,0.006,0.009,0.0001,0.0003,0.0006,0.0009,0.4,0.8,0.04,0.08,0.004,0.008,0.0004,0.0008]\n",
        "ass = [0.01,0.03,0.06,0.09,0.001,0.003,0.006,0.009,0.0001,0.0003,0.0006,0.0009,0.4,0.8,0.04,0.08,0.004,0.008,0.0004,0.0008]\n",
        "for wd in wds:\n",
        "  for le in lerates:\n",
        "    for a in ass:\n",
        "    #train_jodt(le,wd,all_features,webcam_feas,webcam_feas_v,all_labels,webcam_labels,webcam_labels_v,writer)\n",
        "      train(le,wd,a,amazon_feas,dslr_feas,caltech_feas,webcam_feas,amazon_labels,dslr_labels,caltech_labels,webcam_labels,webcam_labels_v,webcam_feas_v,writer)\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}